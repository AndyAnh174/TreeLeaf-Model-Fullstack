from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
import torch.nn as nn
from torchvision import transforms
from torchvision.models import vit_b_16, ViT_B_16_Weights
from PIL import Image
import numpy as np
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage
from dotenv import load_dotenv
import os
from typing import Optional
import io

# Kh·ªüi t·∫°o FastAPI v·ªõi prefix /api/v1
app = FastAPI(
    title="üåø AI N√¥ng Nghi·ªáp API",
    description="API ƒë·ªÉ nh·∫≠n di·ªán b·ªánh l√° c√¢y v√† t∆∞ v·∫•n n√¥ng nghi·ªáp",
    version="1.0.0",
    docs_url="/docs",  # Swagger UI
    redoc_url="/redoc",  # ReDoc alternative
    openapi_url="/openapi.json"  # OpenAPI schema
)

# Create router v·ªõi prefix /api/v1
from fastapi import APIRouter
api_router = APIRouter(prefix="/api/v1")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ================== GLOBAL MODELS ==================
device = None
model = None
transform = None
class_names = None
llm = None
combined_text = None

# ================== LOAD MODELS ==================
def load_model():
    """Load Vision Transformer model"""
    global device, model, transform, class_names
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    try:
        checkpoint = torch.load("best.pt", map_location=device)
        weights = ViT_B_16_Weights.IMAGENET1K_V1
        model_vit = vit_b_16(weights=weights)
        num_classes = 46
        model_vit.heads.head = nn.Linear(model_vit.heads.head.in_features, num_classes)
        model_vit.load_state_dict(checkpoint["model_state_dict"])
        model_vit.to(device)
        model_vit.eval()
        
        transform_weights = weights.transforms()
        class_names_list = [
            'apple_apple_scab', 'apple_black_rot', 'apple_cedar_apple_rust', 'apple_healthy',
            'blueberry_healthy', 'cherry_including_sour_healthy', 'cherry_including_sour_powdery_mildew',
            'corn_maize_cercospora_leaf_spot_gray_leaf_spot', 'corn_maize_common_rust', 'corn_maize_healthy',
            'corn_maize_northern_leaf_blight', 'grape_black_rot', 'grape_esca_black_measles',
            'grape_healthy', 'grape_leaf_blight_isariopsis_leaf_spot', 'mango_anthracnose',
            'mango_bacterial_canker', 'mango_cutting_weevil', 'mango_die_back', 'mango_gall_midge',
            'mango_healthy', 'mango_powdery_mildew', 'mango_sooty_mould',
            'orange_haunglongbing_citrus_greening', 'peach_bacterial_spot', 'peach_healthy',
            'pepper_bell_bacterial_spot', 'pepper_bell_healthy', 'potato_early_blight',
            'potato_healthy', 'potato_late_blight', 'raspberry_healthy', 'soybean_healthy',
            'squash_powdery_mildew', 'strawberry_healthy', 'strawberry_leaf_scorch',
            'tomato_bacterial_spot', 'tomato_early_blight', 'tomato_healthy', 'tomato_late_blight',
            'tomato_leaf_mold', 'tomato_septoria_leaf_spot', 'tomato_spider_mites_two_spotted_spider_mite',
            'tomato_target_spot', 'tomato_tomato_mosaic_virus', 'tomato_tomato_yellow_leaf_curl_virus'
        ]
        
        model = model_vit
        transform = transform_weights
        class_names = class_names_list
        
        return True
    except Exception as e:
        raise Exception(f"Kh√¥ng th·ªÉ t·∫£i model: {e}")

def load_chatbot():
    """Load Chatbot v·ªõi Google Gemini"""
    global llm, combined_text
    
    load_dotenv()
    
    try:
        loader = TextLoader("books/huong_dan_trong_cay.txt", encoding="utf-8")
        documents = loader.load()
        
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = splitter.split_documents(documents)
        combined_text = "\n\n".join([chunk.page_content for chunk in chunks])
        
        llm_instance = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            temperature=0.3,
            google_api_key=os.getenv("GOOGLE_API_KEY")
        )
        
        llm = llm_instance
        return True
    except Exception as e:
        raise Exception(f"Kh√¥ng th·ªÉ t·∫£i chatbot: {e}")

# ================== PREDICT & CHATBOT FUNCTIONS ==================
def predict_disease(image: Image.Image):
    """D·ª± ƒëo√°n b·ªánh t·ª´ ·∫£nh"""
    if model is None:
        raise HTTPException(status_code=503, detail="Model ch∆∞a ƒë∆∞·ª£c load")
    
    try:
        img = image.convert("RGB")
        img_t = transform(img).unsqueeze(0).to(device)
        
        with torch.no_grad():
            outputs = model(img_t)
            probs = torch.softmax(outputs, dim=1).cpu().numpy()[0]
            pred_idx = np.argmax(probs)
        
        return class_names[pred_idx], float(probs[pred_idx])
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói d·ª± ƒëo√°n: {str(e)}")

def chatbot_response(query: str, context_info: Optional[str] = None):
    """Tr·∫£ l·ªùi chatbot"""
    if llm is None:
        raise HTTPException(status_code=503, detail="Chatbot ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
    
    try:
        context = f"Th√¥ng tin b·ªánh c√¢y: {context_info}\n\n" if context_info else ""
        messages = [
            SystemMessage(content="B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ n√¥ng nghi·ªáp. Tr·∫£ l·ªùi th√¢n thi·ªán, d·ªÖ hi·ªÉu cho n√¥ng d√¢n Vi·ªát Nam."),
            HumanMessage(content=f"T√†i li·ªáu:\n{combined_text}\n\n{context}C√¢u h·ªèi: {query}")
        ]
        response = llm.invoke(messages)
        return response.content
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói chatbot: {str(e)}")

# ================== STARTUP ==================
@app.on_event("startup")
async def startup_event():
    """Kh·ªüi t·∫°o models khi server start"""
    print("üîÑ ƒêang load models...")
    try:
        load_model()
        print("‚úÖ Model ƒë√£ load th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ùå L·ªói load model: {e}")
    
    try:
        load_chatbot()
        print("‚úÖ Chatbot ƒë√£ load th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ùå L·ªói load chatbot: {e}")

# ================== API ENDPOINTS ==================

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "üåø AI N√¥ng Nghi·ªáp API",
        "version": "1.0.0",
        "endpoints": {
            "/": "Trang ch·ªß",
            "/health": "Ki·ªÉm tra tr·∫°ng th√°i",
            "/predict": "POST - Nh·∫≠n di·ªán b·ªánh l√° c√¢y",
            "/chat": "POST - Chatbot t∆∞ v·∫•n",
            "/class-names": "GET - Danh s√°ch b·ªánh",
            "/statistics": "GET - Th·ªëng k√™ & Bi·ªÉu ƒë·ªì",
            "/docs": "Swagger documentation"
        }
    }

@app.get("/health")
async def health_check():
    """Ki·ªÉm tra tr·∫°ng th√°i h·ªá th·ªëng"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "chatbot_loaded": llm is not None,
        "device": str(device) if device else None
    }

# Register router v·ªõi prefix /api/v1
api_router.get("/health")(health_check)

async def predict_route(file: UploadFile = File(...)):
    """
    Nh·∫≠n di·ªán b·ªánh l√° c√¢y t·ª´ ·∫£nh
    
    - **file**: File ·∫£nh (JPG, JPEG, PNG)
    
    Returns:
    - disease: T√™n b·ªánh ƒë∆∞·ª£c ph√°t hi·ªán
    - confidence: ƒê·ªô tin c·∫≠y (0-1)
    """
    # Ki·ªÉm tra file type
    if not file.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="File ph·∫£i l√† ·∫£nh (JPG, PNG, JPEG)")
    
    try:
        # ƒê·ªçc v√† x·ª≠ l√Ω ·∫£nh
        contents = await file.read()
        image = Image.open(io.BytesIO(contents))
        
        # D·ª± ƒëo√°n
        disease, confidence = predict_disease(image)
        
        return {
            "success": True,
            "disease": disease,
            "confidence": confidence,
            "confidence_percent": f"{confidence:.1%}"
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω ·∫£nh: {str(e)}")

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    return await predict_route(file)

# Register v·ªõi router
api_router.post("/predict")(predict_route)

class ChatRequest(BaseModel):
    """Request model cho chatbot"""
    query: str
    context: Optional[str] = None

async def chat_route(request: ChatRequest):
    """
    Chatbot t∆∞ v·∫•n n√¥ng nghi·ªáp
    
    - **query**: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
    - **context**: Ng·ªØ c·∫£nh (th√¥ng tin v·ªÅ b·ªánh c√¢y n·∫øu c√≥)
    
    Returns:
    - response: C√¢u tr·∫£ l·ªùi t·ª´ AI
    """
    try:
        response = chatbot_response(request.query, request.context)
        
        return {
            "success": True,
            "response": response
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói chatbot: {str(e)}")

@app.post("/chat")
async def chat(request: ChatRequest):
    return await chat_route(request)

# Register v·ªõi router
api_router.post("/chat")(chat_route)

async def get_class_names_route():
    """L·∫•y danh s√°ch t√™n c√°c class b·ªánh"""
    return {
        "success": True,
        "classes": class_names,
        "total": len(class_names) if class_names else 0
    }

@app.get("/class-names")
async def get_class_names():
    return await get_class_names_route()

# Register v·ªõi router
api_router.get("/class-names")(get_class_names_route)

async def get_statistics_route():
    """
    L·∫•y d·ªØ li·ªáu th·ªëng k√™ cho bi·ªÉu ƒë·ªì
    
    Returns:
    - metrics: C√°c ch·ªâ s·ªë t·ªïng quan (t·ªïng ph√¢n t√≠ch, ƒë·ªô ch√≠nh x√°c, c√¢u h·ªèi chatbot, ng∆∞·ªùi d√πng)
    - weekly_analysis: D·ªØ li·ªáu ph√¢n t√≠ch theo tu·∫ßn
    - disease_distribution: Ph√¢n b·ªë b·ªánh
    """
    # D·ªØ li·ªáu m·∫´u (c√≥ th·ªÉ thay th·∫ø b·∫±ng database th·ª±c t·∫ø)
    metrics = {
        "total_analysis": 1247,
        "total_analysis_today": 23,
        "accuracy": 95.8,
        "accuracy_delta": 2.1,
        "chatbot_questions": 856,
        "chatbot_questions_today": 45,
        "total_users": 234,
        "new_users": 12
    }
    
    # D·ªØ li·ªáu ph√¢n t√≠ch theo tu·∫ßn
    weekly_analysis = {
        "labels": ["Th·ª© 2", "Th·ª© 3", "Th·ª© 4", "Th·ª© 5", "Th·ª© 6", "Th·ª© 7", "CN"],
        "data": [23, 45, 56, 78, 32, 67, 89]
    }
    
    # Ph√¢n b·ªë b·ªánh
    disease_distribution = {
        "labels": ["B·ªánh ƒë·ªëm l√°", "B·ªánh h√©o xanh", "B·ªánh th·ªëi r·ªÖ", "Kh·ªèe m·∫°nh"],
        "data": [35, 28, 22, 15]
    }
    
    return {
        "success": True,
        "metrics": metrics,
        "weekly_analysis": weekly_analysis,
        "disease_distribution": disease_distribution
    }

@app.get("/statistics")
async def get_statistics():
    return await get_statistics_route()

# Register v·ªõi router
api_router.get("/statistics")(get_statistics_route)

# Include router v√†o app
app.include_router(api_router)

# Mount FastAPI docs t·∫°i root (s·∫Ω ƒë∆∞·ª£c proxy b·ªüi nginx)
@app.get("/api/v1/docs")
async def docs_redirect():
    """Redirect to FastAPI Swagger UI"""
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url="/docs")

# Ch·∫°y server
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

